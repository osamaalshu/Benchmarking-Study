============================================================
                    UNET MODEL EVALUATION REPORT
============================================================
Date: August 2, 2025
Model: UNet (3-class segmentation)
Dataset: Cell Microscopy Images
Evaluation Threshold: 0.5 (IoU)

============================================================
üìä EXECUTIVE SUMMARY
============================================================
The UNet model achieved moderate performance on the cell segmentation task:
- Mean F1 Score: 0.3524 ¬± 0.2316
- Mean Dice Score: 0.7030 ¬± 0.1701
- Overall cell detection rate: 16.27%
- False positive rate: 64.71%

============================================================
üìà DETAILED METRICS
============================================================

OVERALL PERFORMANCE (101 test images):
- Mean F1 Score:     0.3524 ¬± 0.2316
- Median F1 Score:   0.3433
- Mean Precision:    0.3445 ¬± 0.2234
- Mean Recall:       0.4110 ¬± 0.2868
- Mean Dice:         0.7030 ¬± 0.1701

CELL DETECTION STATISTICS:
- Total True Cells:      135,477
- Total Predicted:       62,466
- True Positives (TP):   22,045
- False Negatives (FN):  113,432
- False Positives (FP):  40,421

DERIVED METRICS:
- Overall Precision:     0.3529 (35.29% of predictions are correct)
- Overall Recall:        0.1627 (16.27% of true cells detected)
- Overall F1:            0.2227 (harmonic mean of precision and recall)

PERFORMANCE RANGES:
- F1 Score Range:     0.0000 - 0.8889
- Precision Range:    0.0000 - 0.8791
- Recall Range:       0.0000 - 1.0000
- Dice Range:         0.1303 - 0.9293

============================================================
üîç PERFORMANCE ANALYSIS
============================================================

STRENGTHS:
1. Good Dice Score (0.70): Indicates decent overlap between predictions and ground truth
2. Wide Performance Range: Some images achieve excellent results (F1 up to 0.89)
3. Balanced Precision/Recall: Both metrics around 0.35-0.41
4. Successful Training: 57.4% loss reduction during training

LIMITATIONS:
1. Low Overall Recall (16.27%): Model misses 83.73% of true cells
2. High False Positive Rate: 64.71% of predictions are incorrect
3. Inconsistent Performance: High standard deviation indicates variability
4. Cell Detection Challenge: Only detects 22,045 out of 135,477 cells

============================================================
üìä PER-IMAGE ANALYSIS
============================================================

BEST PERFORMING IMAGES (Top 5 by F1 Score):
1. cell_00009_label.tiff: F1=0.8872, Precision=0.8791, Recall=0.8955
2. cell_00005_label.tiff: F1=0.8485, Precision=0.7368, Recall=1.0000
3. cell_00004_label.tiff: F1=0.5769, Precision=0.4054, Recall=1.0000
4. cell_00002_label.tiff: F1=0.4646, Precision=0.5349, Recall=0.4107
5. cell_00003_label.tiff: F1=0.4444, Precision=0.4000, Recall=0.5000

WORST PERFORMING IMAGES (Bottom 5 by F1 Score):
1. cell_00007_label.tiff: F1=0.1111, Precision=0.1176, Recall=0.1053
2. cell_00001_label.tiff: F1=0.1000, Precision=0.1429, Recall=0.0769
3. cell_00006_label.tiff: F1=0.2344, Precision=0.1630, Recall=0.4167
4. cell_00008_label.tiff: F1=0.3750, Precision=0.4286, Recall=0.3333

============================================================
üéØ INTERPRETATION & INSIGHTS
============================================================

MODEL BEHAVIOR:
1. The model shows good segmentation quality (Dice=0.70) but struggles with cell counting
2. High variability suggests sensitivity to image characteristics
3. Better performance on images with fewer, larger cells
4. Struggles with dense cell populations and small cells

TASK DIFFICULTY:
1. Cell segmentation is inherently challenging due to:
   - Overlapping cells
   - Variable cell sizes and shapes
   - Background noise and artifacts
   - Complex cell boundaries

COMPARISON CONTEXT:
- F1 score of 0.35 is moderate for cell segmentation tasks
- Dice score of 0.70 indicates good pixel-level accuracy
- Performance is suitable as a baseline for comparison with UNETR and SwinUNETR

============================================================
üìã RECOMMENDATIONS
============================================================

FOR IMPROVEMENT:
1. Data Augmentation: Increase variety in training data
2. Post-processing: Implement morphological operations
3. Multi-scale Training: Use different input resolutions
4. Loss Function: Experiment with focal loss or boundary-aware losses

FOR BENCHMARKING:
1. Compare with UNETR and SwinUNETR using same evaluation protocol
2. Analyze performance across different cell types and densities
3. Consider ensemble methods combining multiple architectures
4. Evaluate computational efficiency (training/inference time)

============================================================
üìÅ TECHNICAL DETAILS
============================================================

EVALUATION SETUP:
- Test Images: 101 microscopy images
- Ground Truth: Manual cell annotations
- Evaluation Metric: IoU threshold of 0.5
- Post-processing: Morphological operations applied
- Hardware: Apple M3 Max with MPS acceleration

MODEL CONFIGURATION:
- Architecture: UNet with 5 levels
- Input Channels: 3 (RGB)
- Output Classes: 3 (background, interior, boundary)
- Training: 99 epochs with early stopping
- Validation Dice: 0.630 (best)

============================================================
CONCLUSION
============================================================
The UNet model provides a solid baseline for cell segmentation with moderate performance. While the overall F1 score of 0.35 indicates room for improvement, the Dice score of 0.70 shows good pixel-level accuracy. The model successfully demonstrates the feasibility of deep learning for cell segmentation and establishes a benchmark for comparison with more advanced architectures like UNETR and SwinUNETR.

The evaluation reveals the complexity of cell segmentation tasks and highlights the need for robust evaluation metrics that consider both pixel-level accuracy and object-level detection performance.

============================================================
End of Evaluation Report
============================================================ 