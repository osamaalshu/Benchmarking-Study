\begin{table}[htbp]
\centering
\caption{Error Analysis Summary - Average Error Counts Per Image}
\label{tab:error_analysis}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{False Negatives} & \textbf{False Positives} & \textbf{Splits} & \textbf{Merges} & \textbf{Total Errors} \\
\midrule
\textbf{MAUNet-Ensemble} & \textbf{241.0} & \textbf{109.0} & \textbf{1.3} & \textbf{28.3} & \textbf{379.6} \\
\textbf{MAUNet-Wide} & \textbf{241.0} & 139.1 & \textbf{1.3} & 35.2 & 416.6 \\
\textbf{MAUNet-ResNet50} & 254.7 & 134.6 & \textbf{0.9} & 36.9 & 427.1 \\
nnU-Net & 296.4 & 272.7 & 3.6 & 47.3 & 620.0 \\
U-Net & 319.6 & 268.0 & 3.1 & 53.1 & 643.8 \\
LSTM-UNet & 315.8 & 389.0 & 1.7 & 41.8 & 748.3 \\
SAC & 453.5 & 179.0 & 0.1 & 8.2 & 640.8 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Note: Bold values indicate best (lowest) performance in each error category. Analysis based on 100 test images with average 454.4 ground truth cells per image.
\item False Negatives: Missed cells that should have been detected
\item False Positives: Incorrect detections (artifacts segmented as cells)
\item Splits: Ground truth cells divided into multiple predictions (over-segmentation)
\item Merges: Multiple ground truth cells combined into single prediction (under-segmentation)
\end{tablenotes}
\end{table}
