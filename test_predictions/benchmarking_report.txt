# Cell Segmentation Model Benchmarking Results

Generated on: 2025 08 10 20:06:58

## Executive Summary

### Performance Summary (Threshold = 0.5)

  Model               Mean F1     Median F1     Std F1     Mean Dice     Mean Precision     Mean Recall  
 :                           :             :          :             :                  :               : 
  UNET                 0.2978        0.2414     0.2223        0.6649             0.3065          0.3357  
  NNUNET               0.3826        0.3815     0.2236        0.693              0.3611          0.4802  
  SAC                  0.0037        0          0.0082        0.1247             0.0067          0.0107  
  LSTMUNET             0.2898        0.3163     0.2068        0.6424             0.2593          0.4552  
  MAUNET               0.5934        0.6597     0.2558        0.7352             0.5979          0.6066  
  MAUNET_ENSEMBLE      0.6567        0.7421     0.264         0.7315             0.6722          0.6511  

    **Best Performing Model**: MAUNET_ENSEMBLE (Mean F1: 0.6567)

## Performance Visualizations

### Overall Performance Comparison

![Performance Comparison](../visualization_results/performance_comparison.png)

*Figure 1: Comprehensive performance comparison across all metrics (F1, Dice, Precision, Recall)*

### F1 Score Across Different IoU Thresholds

![F1 Threshold Comparison](../visualization_results/f1_threshold_comparison.png)

*Figure 2: F1 Score performance at different IoU thresholds (0.5, 0.7, 0.9) showing model robustness*

### Training Information Comparison

![Training Comparison](../visualization_results/training_comparison.png)

*Figure 3: Training epochs and final loss comparison across models*

## Training Information

### Model Sources and Repositories

  **UNET**: MONAI Framework   Built in MONAI implementation

  **NNUNET**: MIC DKFZ   [https://github.com/mic dkfz/nnunet](https://github.com/mic dkfz/nnunet)

  **SAC**: Authors via Email   Code provided by authors via email

  **LSTMUNET**: GitLab   shaked0   [https://gitlab.com/shaked0/lstmUnet](https://gitlab.com/shaked0/lstmUnet)

  **MAUNET**: NeurIPS 2022 Challenge   [https://github.com/Woof6/neurips22 cellseg_saltfish](https://github.com/Woof6/neurips22 cellseg_saltfish)

  **MAUNET_ENSEMBLE**: NeurIPS 2022 Challenge (Ensemble)   [https://github.com/Woof6/neurips22 cellseg_saltfish](https://github.com/Woof6/neurips22 cellseg_saltfish)

### Model Architectures and Training Parameters

  Model             Architecture                                  Source                              Repository                                            Batch Size       Learning Rate     Input Size     Optimizer     Total Epochs     Final Loss       Best Val Dice         Training Status    
 :                 :                                             :                                   :                                                     :                :                 :              :             :                :                :                     :                   
  UNET              U Net with ResNet blocks                      MONAI Framework                     Built in MONAI implementation                         8                6e 4              256x256        AdamW         58               0.7364           0.6130                Completed          
  NNUNET            nnU Net (No New U Net)                        MIC DKFZ                            https://github.com/mic dkfz/nnunet                    8                6e 4              256x256        AdamW         86               0.5139           0.6744                Completed          
  SAC               Segment Anything + Custom Head                Authors via Email                   Code provided by authors via email                    2                6e 4              256x256        AdamW         52               1.3622           0.2128                Completed          
  LSTMUNET          U Net with LSTM layers                        GitLab   shaked0                    https://gitlab.com/shaked0/lstmUnet                   8                6e 4              256x256        AdamW         39               0.9203           0.5898                Completed          
  MAUNET            MAU Net with ResNet50 backbone                NeurIPS 2022 Challenge              https://github.com/Woof6/neurips22 cellseg_saltfish   8                6e 4              256x256        AdamW         194              0.3911           N/A (No validation)   Completed          
  MAUNET_ENSEMBLE   MAU Net Ensemble (ResNet50 + Wide ResNet50)   NeurIPS 2022 Challenge (Ensemble)   https://github.com/Woof6/neurips22 cellseg_saltfish   N/A (Ensemble)   N/A (Ensemble)    256x256        AdamW         N/A (Ensemble)   N/A (Ensemble)   N/A (Ensemble)        Ensemble Model     


### Training Summary

  **Total Models Trained**: 5

  **Most Epochs**: 194 (MAUNET)

  **Best Training Validation Dice**: 0.6744 (NNUNET)

  **Optimizer**: AdamW (all models)

  **Learning Rate**: 6e 4 (all models)

## Sample Segmentation Results

### Qualitative Comparison

The following images show qualitative comparisons between ground truth and model predictions:

#### Sample 1: cell_00036

**UNET**: ![unet cell_00036](../visualization_results/unet/cell_00036_comparison.png)

**NNUNET**: ![nnunet cell_00036](../visualization_results/nnunet/cell_00036_comparison.png)

**SAC**: ![sac cell_00036](../visualization_results/sac/cell_00036_comparison.png)

**LSTMUNET**: ![lstmunet cell_00036](../visualization_results/lstmunet/cell_00036_comparison.png)

**MAUNET**: ![maunet cell_00036](../visualization_results/maunet/cell_00036_comparison.png)

**MAUNET_ENSEMBLE**: ![maunet_ensemble cell_00036](../visualization_results/maunet_ensemble/cell_00036_comparison.png)

   

#### Sample 2: cell_00029

**UNET**: ![unet cell_00029](../visualization_results/unet/cell_00029_comparison.png)

**NNUNET**: ![nnunet cell_00029](../visualization_results/nnunet/cell_00029_comparison.png)

**SAC**: ![sac cell_00029](../visualization_results/sac/cell_00029_comparison.png)

**LSTMUNET**: ![lstmunet cell_00029](../visualization_results/lstmunet/cell_00029_comparison.png)

**MAUNET**: ![maunet cell_00029](../visualization_results/maunet/cell_00029_comparison.png)

**MAUNET_ENSEMBLE**: ![maunet_ensemble cell_00029](../visualization_results/maunet_ensemble/cell_00029_comparison.png)

   

### Individual Segmentation Examples

![cell_00001 Segmentation](../visualization_results/cell_00001_visualization.png)

*Cell 00001   Original image, ground truth, and prediction comparison*

![cell_00002 Segmentation](../visualization_results/cell_00002_visualization.png)

*Cell 00002   Original image, ground truth, and prediction comparison*


## Detailed Results by Model

### UNET

#### Performance Across Thresholds

    Threshold     Mean F1     Mean Dice     Mean Precision     Mean Recall     Total Samples  
             :           :             :                  :               :                 : 
          0.5      0.2978        0.6649             0.3065          0.3357                65  
          0.7      0.1327        0.6649             0.1357          0.1513                65  
          0.9      0.0084        0.6649             0.0067          0.0125                65  

#### Top 5 Performing Images

  names                       F1     dice  
 :                              :        : 
  cell_00015_label.tiff   1        0.918   
  cell_00011_label.tiff   0.9655   0.8499  
  cell_00017_label.tiff   0.9362   0.8554  
  cell_00051_label.tiff   0.6898   0.7786  
  cell_00065_label.tiff   0.5801   0.7956  

#### Bottom 5 Performing Images

  names                       F1     dice  
 :                              :        : 
  cell_00036_label.tiff   0.0073   0.3396  
  cell_00072_label.tiff   0.0108   0.2043  
  cell_00038_label.tiff   0.0131   0.3153  
  cell_00028_label.tiff   0.0139   0.3249  
  cell_00037_label.tiff   0.0214   0.2928  


### NNUNET

#### Performance Across Thresholds

    Threshold     Mean F1     Mean Dice     Mean Precision     Mean Recall     Total Samples  
             :           :             :                  :               :                 : 
          0.5      0.3826         0.693             0.3611          0.4802               101  
          0.7      0.2071         0.693             0.1909          0.2809               101  
          0.9      0.025          0.693             0.0213          0.0422               101  

#### Top 5 Performing Images

  names                       F1     dice  
 :                              :        : 
  cell_00009_label.tiff   0.9159   0.9236  
  cell_00098_label.tiff   0.84     0.8232  
  cell_00005_label.tiff   0.8116   0.865   
  cell_00094_label.tiff   0.7857   0.8158  
  cell_00085_label.tiff   0.7356   0.7991  

#### Bottom 5 Performing Images

  names                       F1     dice  
 :                              :        : 
  cell_00076_label.tiff   0        0.7024  
  cell_00100_label.tiff   0.0027   0.0217  
  cell_00077_label.tiff   0.0032   0.518   
  cell_00041_label.tiff   0.0133   0.0859  
  cell_00078_label.tiff   0.02     0.6967  


### SAC

#### Performance Across Thresholds

    Threshold     Mean F1     Mean Dice     Mean Precision     Mean Recall     Total Samples  
             :           :             :                  :               :                 : 
          0.5      0.0037        0.1247             0.0067          0.0107               101  
          0.7      0.0002        0.1247             0.0002          0.0003               101  
          0.9      0             0.1247             0               0                    101  

#### Top 5 Performing Images

  names                       F1     dice  
 :                              :        : 
  cell_00094_label.tiff   0.044    0.3617  
  cell_00001_label.tiff   0.0408   0.273   
  cell_00016_label.tiff   0.029    0.3127  
  cell_00022_label.tiff   0.0288   0.4283  
  cell_00013_label.tiff   0.027    0.39    

#### Bottom 5 Performing Images

  names                     F1     dice  
 :                            :        : 
  cell_00002_label.tiff      0   0       
  cell_00003_label.tiff      0   0.0257  
  cell_00004_label.tiff      0   0.0155  
  cell_00006_label.tiff      0   0       
  cell_00007_label.tiff      0   0.2598  


### LSTMUNET

#### Performance Across Thresholds

    Threshold     Mean F1     Mean Dice     Mean Precision     Mean Recall     Total Samples  
             :           :             :                  :               :                 : 
          0.5      0.2898        0.6424             0.2593          0.4552               101  
          0.7      0.1094        0.6424             0.0921          0.2098               101  
          0.9      0.0023        0.6424             0.0016          0.0274               101  

#### Top 5 Performing Images

  names                       F1     dice  
 :                              :        : 
  cell_00009_label.tiff   0.844    0.8672  
  cell_00085_label.tiff   0.6735   0.8381  
  cell_00096_label.tiff   0.6355   0.7801  
  cell_00084_label.tiff   0.6346   0.8248  
  cell_00086_label.tiff   0.6316   0.8312  

#### Bottom 5 Performing Images

  names                       F1     dice  
 :                              :        : 
  cell_00076_label.tiff   0        0.3973  
  cell_00077_label.tiff   0        0.7563  
  cell_00078_label.tiff   0        0.6804  
  cell_00070_label.tiff   0.005    0.0607  
  cell_00074_label.tiff   0.0059   0.548   


### MAUNET

#### Performance Across Thresholds

    Threshold     Mean F1     Mean Dice     Mean Precision     Mean Recall     Total Samples  
             :           :             :                  :               :                 : 
          0.5      0.5934        0.7352             0.5979          0.6066               101  
          0.7      0.3713        0.7352             0.3695          0.385                101  
          0.9      0.0568        0.7352             0.0547          0.0618               101  

#### Top 5 Performing Images

  names                       F1     dice  
 :                              :        : 
  cell_00005_label.tiff   1        0.9285  
  cell_00011_label.tiff   1        0.896   
  cell_00015_label.tiff   1        0.9323  
  cell_00009_label.tiff   0.9689   0.9359  
  cell_00004_label.tiff   0.9677   0.9487  

#### Bottom 5 Performing Images

  names                       F1     dice  
 :                              :        : 
  cell_00100_label.tiff   0        0.0131  
  cell_00078_label.tiff   0.006    0.105   
  cell_00074_label.tiff   0.0098   0.0212  
  cell_00099_label.tiff   0.0208   0.1297  
  cell_00077_label.tiff   0.0211   0.5852  


### MAUNET_ENSEMBLE

#### Performance Across Thresholds

    Threshold     Mean F1     Mean Dice     Mean Precision     Mean Recall     Total Samples  
             :           :             :                  :               :                 : 
          0.5      0.6567        0.7315             0.6722          0.6511               101  
          0.7      0.3896        0.7315             0.4017          0.3834               101  
          0.9      0.0664        0.7315             0.0675          0.0662               101  

#### Top 5 Performing Images

  names                     F1     dice  
 :                            :        : 
  cell_00004_label.tiff      1   0.9422  
  cell_00005_label.tiff      1   0.9277  
  cell_00011_label.tiff      1   0.8987  
  cell_00015_label.tiff      1   0.9316  
  cell_00042_label.tiff      1   0.9045  

#### Bottom 5 Performing Images

  names                     F1     dice  
 :                            :        : 
  cell_00072_label.tiff      0   0.0383  
  cell_00073_label.tiff      0   0.0956  
  cell_00074_label.tiff      0   0.0009  
  cell_00078_label.tiff      0   0.0074  
  cell_00100_label.tiff      0   0.0191  


## Dataset Analysis

### Performance vs. Ground Truth Cell Count

#### UNET   Performance by Cell Count

  GT Count Range           mean     count  
 :                             :         : 
  1 5                nan                0  
  6 10                 0.375            1  
  11 20                0.344544         9  
  21 50                0.331288         8  
  51 100               0.107433         6  
  100+                 0.256804        25  


#### NNUNET   Performance by Cell Count

  GT Count Range           mean     count  
 :                             :         : 
  1 5                nan                0  
  6 10                 0.387467         3  
  11 20                0.377541        17  
  21 50                0.468568        22  
  51 100               0.371245        11  
  100+                 0.307655        29  


#### SAC   Performance by Cell Count

  GT Count Range             mean     count  
 :                               :         : 
  1 5                nan                  0  
  6 10                 0                  3  
  11 20                0.00850588        17  
  21 50                0.00404545        22  
  51 100               0                 11  
  100+                 0.00294138        29  


#### LSTMUNET   Performance by Cell Count

  GT Count Range           mean     count  
 :                             :         : 
  1 5                nan                0  
  6 10                 0.146333         3  
  11 20                0.184441        17  
  21 50                0.352859        22  
  51 100               0.287991        11  
  100+                 0.289528        29  


#### MAUNET   Performance by Cell Count

  GT Count Range           mean     count  
 :                             :         : 
  1 5                nan                0  
  6 10                 0.419867         3  
  11 20                0.638565        17  
  21 50                0.660086        22  
  51 100               0.513636        11  
  100+                 0.533503        29  


#### MAUNET_ENSEMBLE   Performance by Cell Count

  GT Count Range           mean     count  
 :                             :         : 
  1 5                nan                0  
  6 10                 0.6627           3  
  11 20                0.778018        17  
  21 50                0.763509        22  
  51 100               0.589509        11  
  100+                 0.54279         29  


## Model Comparison

### Metrics Comparison (Threshold = 0.5)

  Metric         UNET     NNUNET      SAC     LSTMUNET     MAUNET     MAUNET_ENSEMBLE  
 :                   :          :        :            :          :                   : 
  F1 Score     0.2978     0.3826   0.0037       0.2898     0.5934              0.6567  
  Dice Score   0.6649     0.693    0.1247       0.6424     0.7352              0.7315  
  Precision    0.3065     0.3611   0.0067       0.2593     0.5979              0.6722  
  Recall       0.3357     0.4802   0.0107       0.4552     0.6066              0.6511  

### Statistical Significance Analysis

#### F1 Score Descriptive Statistics

  Model   Mean ± Std   Median   95% CI   Range  
                                                
  UNET   0.2978 ± 0.2206   0.2414   [0.0122, 0.9479]   [0.0073, 1.0000]  
  NNUNET   0.3826 ± 0.2225   0.3815   [0.0083, 0.7986]   [0.0000, 0.9159]  
  SAC   0.0037 ± 0.0081   0.0000   [0.0000, 0.0289]   [0.0000, 0.0440]  
  LSTMUNET   0.2898 ± 0.2057   0.3163   [0.0025, 0.6351]   [0.0000, 0.8440]  
  MAUNET   0.5934 ± 0.2546   0.6597   [0.0153, 0.9845]   [0.0000, 1.0000]  
  MAUNET_ENSEMBLE   0.6567 ± 0.2627   0.7421   [0.0000, 1.0000]   [0.0000, 1.0000]  


#### F1 Score ANOVA Results

  **F statistic**: 121.1493

  **p value**: 0.000000

  **Significant**: Yes


#### F1 Score Kruskal Wallis Results

  **H statistic**: 322.5016

  **p value**: 0.000000

  **Significant**: Yes


#### Key Pairwise Comparisons (F1 Score)

  Comparison   T test p value   Mann Whitney p value   Cohen's d   Effect Size  
                                                                               
  MAUNET VS MAUNET ENSEMBLE   0.085021   0.024466    0.2436   Small  
  NNUNET VS MAUNET ENSEMBLE   0.000000   0.000000    1.1203   Large  
  UNET VS MAUNET ENSEMBLE   0.000000   0.000000    1.4437   Large  


#### Dice Score Descriptive Statistics

  Model   Mean ± Std   Median   95% CI  
                                        
  UNET   0.6649 ± 0.2052   0.7172   [0.1818, 0.9273]  
  NNUNET   0.6930 ± 0.1610   0.7366   [0.2339, 0.8939]  
  SAC   0.1247 ± 0.1258   0.0723   [0.0000, 0.3986]  
  LSTMUNET   0.6424 ± 0.1693   0.6804   [0.0800, 0.8576]  
  MAUNET   0.7352 ± 0.1782   0.7834   [0.1174, 0.9304]  
  MAUNET_ENSEMBLE   0.7315 ± 0.2075   0.7959   [0.0175, 0.9296]  



## Recommendations

Based on the benchmarking results:

1. **MAUNET_ENSEMBLE** shows the best overall performance with highest mean F1 score

2. Consider using threshold = 0.5 for optimal balance between precision and recall

3. Models perform better on images with moderate cell counts (10 50 cells)

4. Further training or fine tuning may improve performance on densely populated images
